% This LaTeX was auto-generated from MATLAB code.
% To make changes, update the MATLAB code and republish this document.

\documentclass[a4paper, 12pt]{article}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amssymb}

\pagestyle{fancy}
\fancyhf{}
\rhead{Joseph Free \\ MATH 3261 \\ Project 6}
\lhead{Eigenvalue Problem: The Power Method }
\addtolength{\headheight}{41.7pt}
\cfoot{\thepage}

\begin{document}
\section{Problem}

In this project we seek to determine the dominant (or largest) eigenvalue and corresponding eigenvector of the matrix

$$
A =
\begin{bmatrix}
	4 & 1 & 1 & 1\\
	1 & 3 & -1 & 1\\
	1 & -1 & 2 & 0\\
	1 & 1 & 0 & 2\\
\end{bmatrix}.
$$

In general, if A is an $n \times n$ matrix and $x$ is an $n \times 1$ vector, we seek all scalars $\lambda$ and vectors $x$ such that the equation \begin{equation} Ax = \lambda x \end{equation} holds. In such a situation, we say the scalar $\lambda$ is an eigenvalue of A, and $x$ is the eigenvector corresponding to $\lambda$, respectively.

In order to determine $\lambda$ and $x$, we make the requirement that $x \neq 0$, and observe that solving $Ax = \lambda x$ is equivalent to solving \begin{equation}(A-I\lambda)x = 0.\end{equation} Then, from the assumption that $x \neq 0$, we conclude that the matrix $A-I\lambda = 0$, and hence must be singular. Therefore, it follows that $det(A-I\lambda) = 0.$ 

Since A is $n \times n$, from the computation of the determinant we obtain an $n$th degree polynomial in $\lambda$, which we call the characteristic polynomial of A. The roots of this polynomial are precisely the eigenvalues of A. Once we've obtain a given eigenvalue, we need only return to equation (2) in order to determine the corresponding eigenvector.

It is easy to see that one method of obtaining the dominant eigenvalue of our matrix A is finding the roots of its characteristic polynomial and ordering them by size. However, it is just an easy to see that as the size of A increases, so too does the difficulty of finding the roots of the characteristic polynomial. Since we are dealing with a $4 \times 4$ matrix, we would expect to be finding the roots of a fourth degree polynomial, which might be tedious. Thus, we will utilize an iterative method known as the Power Method to approximate the dominant eigenvalue.

In order to proceed with the Power Method, we make two basic assumptions about the eigenvalues and eigenvectors. Namely, that the eigenvalues of A may be ordered in such a way that $\vert \lambda_1 \vert > \vert \lambda_2 \vert \geq \hdots \geq \vert \lambda_n \vert$ and the corresponding eigenvectors $u_i$, $1 \leq i \leq n$, form a linearly independent set \{$u_1, u_2, \hdots, u_n$\}.

Since the set of eigenvectors of A is linearly independent, it is possible to express any vector $x \in \mathbb{R}^n $ as a linear combination of the vectors $u_i$. Thus, $$ x = \sum_{j = 1}^{n} b_ju_j. $$ Then by successive multiplication by the matrix A, we can generate the sequence

$$
\begin{array}{ccccc}

y_0 = & Ax  & = & \sum_{j = 1}^{n} b_jAu_j & = \sum_{j = 1}^{n} b_j\lambda_ju_j \\
y_1 = & A^2x  & = & \sum_{j = 1}^{n} b_jA^2u_j & = \sum_{j = 1}^{n} b_j\lambda^2_ju_j \\
y_3 = & A^3x  & = & \sum_{j = 1}^{n} b_jA^3u_j & = \sum_{j = 1}^{n} b_j\lambda^3_ju_j \\
 & \vdots & & \\
 & \vdots & & \\
y_k = & A^kx  & = & \sum_{j = 1}^{n} b_jA^ku_j & = \sum_{j = 1}^{n} b_j\lambda^k_ju_j \\
\end{array}.
$$

By factoring out $\lambda^k_1$ out of the right hand side of the last equation, we obtain \begin{equation}y_k = \lambda^k_1 \sum_{j = 1}^{n} b_j \left[  \frac{\lambda_j}{\lambda_1} \right]  ^ku_j. \end{equation}

Expanding (3), we see that

$$y_k = \lambda^k_1 \sum_{j = 1}^{n} b_j \left[  \frac{\lambda_j}{\lambda_1} \right]  ^ku_j. 
= \lambda^k_1\left( b_1u_1 + b_2\left[  \frac{\lambda_2}{\lambda_1}\right]^ku_2 + \hdots + b_n\left[  \frac{\lambda_n}{\lambda_1}\right]^ku_n\right)
$$

It stands to reason now that since $\lambda_1$ is the largest of all the eigenvalues of A, the quantity  $\left[ \frac{\lambda_j}{\lambda_1} \right] ^k$ must be less than one. Thus, if we take the limit as $k \longrightarrow \infty$, all the terms in (3) but the first must go to zero, and so we conclude that 

$$
\begin{array}{ccccc}
   
y_k  \longrightarrow \lambda^k_1 b_1u_1 &  & as & &  k \longrightarrow \infty

\end{array}
$$

This is the general theory behind the power method. With slight modifications on this general theme, we can obtain a number of ways to approximate $\lambda_1$. In  the source code section below, we implement one such way of doing so using an algorithm from Burden and Faires Numerical Analysis 8th Edition.

\pagebreak
\input{source.txt}

\section{Conclusion}

From the results section above, we see that given A as defined at the beginning of this paper and initial guess vector $x_0 = (1,-2,0,3)^t$, the power method finds $\lambda_1 \approx 5.2359$ with corresponding eigenvector $x \approx (1.0000, .6178, .1182, .4999)^t$. This value is within .0001 of the actual dominant eigenvalue of A. Using a completely different initial guess ( $x_0 = (1,0,1,0)^t$ ) at the same level of tolerance, the method converges to exactly the same eigenvalue/eigenvector.


Furthermore, as evidenced by the first call to pMethod\_hw6, we note that the method takes no less than 20 iterations to reach the desired level of accuracy in this particular case.

\end{document}


